
#############

* tune2fs -m 0 /dev/block # 5% of 2.7tb equates to an extra 135gb of space

#############

 man md.4 mdadm.8.in

 write hole | last written

-------------

 power failure
 marks array dirty
 dirty array forces resync
 assumes data is correct and recalculates parity

 ext4 with fsync and barrier protected journal metadata
 cant manipulate data until metadata has confirmed writes
 fsck.ext4 checks metadata and restores the last known value for any parital writes

* below only applies to ssd/nvme (disable write cache in hdd with hdparm -W0)
  write cache, pools none persistent writes lying to the kernel about fsync+barrier protection
  tldr; dual psu and uninterruptible power supply + backup sync
        removing write cache on ssd/nvme massively reduces life expectency
        data not being written to cant be lost...

 ^ mdadm journal wont stop write cache pooling data and lying about journal integrity
   raid cards wont stop hdd/sdd/nvme write cache pooling data and lying about the journal integrity
   battery protected raid cards only persist write cache across power failure until the battery dies

* --consistency-policy
  resync (repairs all parity blocks in the stripe)
  bitmap (speeds up resync)
  journal (external single point of failure)
  ppl (raid5 only internal journal?)

* --write-journal - clean mdadm up, before cleaning ext4 up ?

* old information implied badblocks was relevent...
   storage firmware removed the need
* old information implied aligning filesystems at block boundries was relevent...
   prevent reading two blocks instead of one
   aligns itself at block boundries < - raid , partition and filesystem levels ?

#############

[maintain]

 mdadm --misc --action=repair /dev/md127
 fsck.ext4 /dev/device
 fstrim -v /mount/point
 e4defrag -v file /path /dev/device

#############

 [mdadm]

 * defaults to left-symmetric parity layout
 - https://www.accs.com/p_and_p/RAID/LinuxRAID.html

# discover

 mdadm --assemble --no-degraded --scan
 mdadm --examine /dev/sda
 mdadm --detail /dev/md127
 mdadm --detail --scan > /etc/mdadm.conf

# create

 mdadm
  --create
  --level={0,1,4,5,6,10} --chunk=512
  --bitmap={none,internal,/path/to/bitmap}
  --write-journal=/dev/block
  --consistency-policy={resync,bitmap,journal,ppl}
  --homehost= --name=storage
  --raid-devices=4 --size=930GB
  /dev/md127 /dev/sda /dev/sdb /dev/sdc /dev/sdd

 mkfs.ext4 -L shared -m0 /dev/md127

# monitor

 watch -tn3 cat /proc/mdstat
 mdadm --monitor --scan --syslog --daemonise
 mdadm --monitor --scan --syslog --oneshot

# repair

 mdadm --misc --action={check,repair} /dev/md127

# stop / fail / remove / add

 mdadm --stop /dev/md127
 mdadm --manage /dev/md127 --fail /dev/sda1
 mdadm --manage /dev/md127 --remove /dev/sda1
 mdadm --manage /dev/md127 --add /dev/sdb1
 watch -tn3 cat /proc/mdstat

# delete

 mdadm --zero-superblock /dev/sd{a-f}

#############

 [e2fsprogs]

-------------

 ext2 no journal
 ext3 has journal, no barrier
 ext4 has journal, with barrier

 mkfs.ext4 -m 0 -L <label> /dev/md127 (/etc/mke2fs.conf)
 fsck.ext4 -pvf /dev/md127
 e4defrag -v /dev/md127
 dumpe2fs /dev/md127 | less
 tune2fs -l /dev/md127 | less

-------------

       mke2fs - create filesystem
    mkfs.ext4 - create filesystem
 mklost+found - create lost and found (pre-allocates disk blocks)

     dumpe2fs - print information about the filesystem
      e2label - change the filesystem label
       e2fsck - repair the filesystem
    fsck.ext4 - repair the filesystem
     e4defrag - defragment the filesystem

      tune2fs - tune the filesystem

-------------

    badblocks - eol
     dumpe2fs + print super block and block group information for /dev/device
   e2freefrag - print number of continous blocks available on /dev/device
       e2fsck + check and repair the filesystem on /dev/device
      e2label + change filesystem label on /dev/device
  e2mmpstatus - print multiple-mount protection status
      e2scrub - bash script for lvm volumes ?
  e2scrub_all - bash script for lvm volumes ?
       e2undo - replay an undo log (undo a failed operation by an e2fsprogs program)
      e4crypt - encryption management for ext4 file systems
     e4defrag + reduce fragmentation of files in the filesystem takes /file /path/to/mount /dev/device
     filefrag - report how badly fragmented a particular file might be takes /file
         fsck - check and repair a linux file system, old wrapper replaced with e2fsck ?
    fsck.ext2 - > e2fsck
    fsck.ext3 - > e2fsck
    fsck.ext4 + > e2fsck
      logsave - > save the output of a command to a logfile
       mke2fs - > create ext 2/3/4 filesystems
    mkfs.ext2 - > mke2fs
    mkfs.ext3 - > mke2fs
    mkfs.ext4 - > mke2fs
 mklost+found + > create lost+found directory (pre-allocates disk blocks)
      tune2fs + set filesystem specific variables

  chattr < - change filesystem specific attributes
  lsattr < - list   filesystem specific attributes

-------------

 technically not e2fsprogs

            fstrim < - replace unused 1s with 0s, cronjob
  mount -o discard < - replace unused 1s with 0s, realtime

-------------

#############

 [tune2fs]

 tune2fs -l /dev/md126 | less -S

 -c 1 (number of mounts before triggering auto fsck ?)
 -C 1 (set the number of times the file system has been mounted)
 -e {continue,remount-ro,panic} error behaviour when errors are detected
 -E extended-options (huge)
 -f force operation
 -g group that can use reserved filesystem blocks (defaults to root)
 -i interval between checks (same as -c defaults to 0 auto fsck)
 -I inode size used by the filesystem
 -j add ext3 journal to the filesystem ?
 -J journal-options (huge)
 -l list contents of the filesystem superblock
 -L volume label (duplicates e2label)
 -m reserved blocks percentage (defaults 5%)
 -M set last mounted directory path /shrug
 -o mount-options (huge)
 -O feature-options (huge)
 -r reserved blocks count
 -Q quota-options
 -T set time last checked ( see -i )
 -u user that can use reserved filesystem blocks (defaults to root)
 -U uuid (clear or set manual/random/time based uuid generation)
 -z write old contents to an undo file (recover with e2undo)

main interesting points
 automating fsck based on number of mounts or time passed
 behaviour when errors are detected
 % of the filesystem to reserve and uid / gid to access reserved blocks

main chunk of functionality is
 -J journal options (assuming defaults? size/fast_commit_size/location)
 -o mount options (hardcode default mount options)
 -O feature options (assuming defaults? verity+fsverity-utils)
 -E extended options (assuming defaults? stride= sripte_width= configure ext4 for use with raid?)

#############

 [hdparam]

 busybox only
 harddrives only (sdparm?)
 hdparm -I /dev/sda
 hdparm -W0 /dev/sda
 not alot worth mentioning ?
 fs readahead ?
 using_dma ?

#############

# read/write test - tl;dr raid5 without bitmap won

-------------

[ -f /tmp/random ] || dd if=/dev/urandom of=/tmp/random bs=4096 count=100000

sync && busybox dd if=/tmp/random of=/storage/a.shared/test0 bs=4096 conv=fsync
sync && busybox dd if=/tmp/random of=/storage/a.shared/test1 bs=4096 conv=fsync
sync && busybox dd if=/tmp/random of=/storage/a.shared/test2 bs=4096 conv=fsync

sync && echo 3 >> /proc/sys/vm/drop_caches && busybox dd if=/storage/a.shared/test0 of=/dev/null bs=4096
sync && echo 3 >> /proc/sys/vm/drop_caches && busybox dd if=/storage/a.shared/test1 of=/dev/null bs=4096
sync && echo 3 >> /proc/sys/vm/drop_caches && busybox dd if=/storage/a.shared/test2 of=/dev/null bs=4096


raid5  chunk=512  bitmap=none     | write0=344MB/s write1=332MB/s write2=353MB/s | read0=675MB/s read0=679MB/s read1=664MB/s
raid5  chunk=512  bitmap=internal | write0=198MB/s write1=254MB/s write2=279MB/s | read0=494MB/s read0=607MB/s read1=669MB/s

raid5  chunk=4096 bitmap=none     | write0=274MB/s write1=265MB/s write2=285MB/s | read0=670MB/s read0=631MB/s read1=621MB/s
raid5  chunk=4096 bitmap=internal | write0=225MB/s write1=202MB/s write2=228MB/s | read0=605MB/s read0=617MB/s read1=630MB/s

raid10 chunk=512  bitmap=none     | write0=239MB/s write1=258MB/s write2=247MB/s | read0=416MB/s read1=465MB/s read2=439MB/s
raid10 chunk=4096 bitmap=none     | write0=319MB/s write1=324MB/s write2=332MB/s | read0=442MB/s read1=440MB/s read2=441MB/s

raid0  chunk=512                  | write0=579MB/s write1=533MB/s write2=578MB/s | read0=955MB/s read1=745MB/s read2=884MB/s
raid0  chunk=4096                 | write0=527MB/s write1=515MB/s write2=503MB/s | read0=733MB/s read1=709MB/s read2=682MB/s

-------------

raid5 without bitmap, destroyed everything
      need to test partial parity log && external journal

raid10 might get speed increase at 8 disks ?
       12 disks should in theory be hitting 500/1000 MB read writes
       whilst halfing storage capacity...

* internal bitmaps make recovery quicker
  at the expense of reduced speed

* chunk size relative to median file size
  alot of small files == smaller chunks increase probability of reads across multiple disks
  alot of large files == larger chunks probably reduce fragmentation

^ chunks are number of 4k linux blocks not the block size itself

-------------

#############

badblocks

  hdd - error detection and correction
  ssd - wear leveling
 nvme - wear leveling

^ even if badblocks reports
  failed reads
  failed write reads
  firmware replaces transparently

* using badblocks is no longer viable

* smartctl allows reading firmware information

#############

[fsverity-utils]

 merkle backed read only filesystem
 mkfs.ext4 -O verity
 tune2fs -O verity
 fsverity 

#############

[fscrypt]

#############

